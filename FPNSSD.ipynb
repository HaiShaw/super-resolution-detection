{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following code is modified based on the original torchcv project. \n",
    "* We are going to use PASACAL VOC12 as dataset.\n",
    "* You could donwload VOC2012 \n",
    "  train/validation: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#devkit\n",
    "* test data: https://pjreddie.com/projects/pascal-voc-dataset-mirror/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from __future__ import print_function\n",
    "from torchcv.models.fpnssd import FPNSSD512\n",
    "from torchcv.models.fpnssd import FPNSSDBoxCoder\n",
    "\n",
    "from torchcv.loss import SSDLoss\n",
    "from torchcv.datasets import ListDataset\n",
    "from torchcv.transforms import resize, random_flip, random_paste, random_crop, random_distort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='PyTorch FPNSSD Training')\n",
    "#parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "#parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "#parser.add_argument('--model', default='./model/fpn.pth', type=str, help='initialized model path')\n",
    "#parser.add_argument('--checkpoint', default='./checkpoint/fpn_ckpt.pth', type=str, help='checkpoint path')\n",
    "#args = parser.parse_args()\n",
    "args = { \"lr\":1e-3,\n",
    "        \"resume\":False,\n",
    "       \"model\":\"./model/fpn.pth\",\n",
    "       \"checkpoint\":\"./checkpoint/fpn_ckpt.pth\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing dataset..')\n",
    "img_size = 512\n",
    "box_coder = FPNSSDBoxCoder()\n",
    "\n",
    "def transform_train(img, boxes, labels):\n",
    "    img = random_distort(img)\n",
    "    if random.random() < 0.5:\n",
    "        img, boxes = random_paste(img, boxes, max_ratio=4, fill=(123,116,103))\n",
    "    img, boxes, labels = random_crop(img, boxes, labels)\n",
    "    img, boxes = resize(img, boxes, size=(img_size,img_size), random_interpolation=True)\n",
    "    img, boxes = random_flip(img, boxes)\n",
    "    img = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "    ])(img)\n",
    "    boxes, labels = box_coder.encode(boxes, labels)\n",
    "    return img, boxes, labels\n",
    "\n",
    "trainset = ListDataset(root='./dataset/VOC12-HR/',    \\\n",
    "                       list_file='./dataset/voc12_train.txt', \\\n",
    "                       transform=transform_train)\n",
    "\n",
    "def transform_test(img, boxes, labels):\n",
    "    img, boxes = resize(img, boxes, size=(img_size,img_size))\n",
    "    img = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "    ])(img)\n",
    "    boxes, labels = box_coder.encode(boxes, labels)\n",
    "    return img, boxes, labels\n",
    "\n",
    "testset = ListDataset(root='./dataset/VOC12-HR-test/',  \\\n",
    "                      list_file='./dataset/voc12_test.txt', \\\n",
    "                        transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "device: cuda\n",
      "\n",
      "Epoch: 0\n",
      "loc_loss: 0.101 | cls_loss: 12.302 | train_loss: 12.403 | avg_loss: 12.403 [1/2141]\n",
      "\n",
      "Test\n",
      "loc_loss: 0.089 | cls_loss: 12.238 | test_loss: 12.327 | avg_loss: 12.327 [1/2141]\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "loc_loss: 0.108 | cls_loss: 12.278 | train_loss: 12.387 | avg_loss: 12.387 [1/2141]\n",
      "\n",
      "Test\n",
      "loc_loss: 0.089 | cls_loss: 12.217 | test_loss: 12.307 | avg_loss: 12.307 [1/2141]\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "loc_loss: 0.086 | cls_loss: 12.244 | train_loss: 12.330 | avg_loss: 12.330 [1/2141]\n",
      "\n",
      "Test\n",
      "loc_loss: 0.089 | cls_loss: 12.206 | test_loss: 12.296 | avg_loss: 12.296 [1/2141]\n",
      "Saving..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-90943c63aeee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-90943c63aeee>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=8)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=8, shuffle=False, num_workers=8)\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"device:\",device)\n",
    "\n",
    "net = FPNSSD512(num_classes=21).to(device)\n",
    "#net.load_state_dict(torch.load(args.model))\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "best_loss = float('inf')  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last epoch\n",
    "\n",
    "if args[\"resume\"]:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = torch.load(args[\"checkpoint\"])\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_loss = checkpoint['loss']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = SSDLoss(num_classes=21)\n",
    "optimizer = optim.SGD(net.parameters(), lr=args['lr'], momentum=0.9, weight_decay=1e-4)\n",
    "#import pdb; pdb.set_trace()\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (inputs, loc_targets, cls_targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        loc_targets = loc_targets.to(device)\n",
    "        cls_targets = cls_targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loc_preds, cls_preds = net(inputs)\n",
    "        loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        print('train_loss: %.3f | avg_loss: %.3f [%d/%d]'\n",
    "              % (loss.item(), train_loss/(batch_idx+1), batch_idx+1, len(trainloader)))\n",
    "\n",
    "def test(epoch):\n",
    "    print('\\nTest')\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, loc_targets, cls_targets) in enumerate(testloader):\n",
    "            inputs = inputs.to(device)\n",
    "            loc_targets = loc_targets.to(device)\n",
    "            cls_targets = cls_targets.to(device)\n",
    "\n",
    "            loc_preds, cls_preds = net(inputs)\n",
    "            loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets)\n",
    "            test_loss += loss.item()\n",
    "            print('test_loss: %.3f | avg_loss: %.3f [%d/%d]'\n",
    "                  % (loss.item(), test_loss/(batch_idx+1), batch_idx+1, len(testloader)))\n",
    "            \n",
    "    # Save checkpoint\n",
    "    global best_loss\n",
    "    test_loss /= len(testloader)\n",
    "    if test_loss < best_loss:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'loss': test_loss,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(os.path.dirname(args['checkpoint'])):\n",
    "            os.mkdir(os.path.dirname(args['checkpoint']))\n",
    "        torch.save(state, args['checkpoint'])\n",
    "        best_loss = test_loss\n",
    "    \n",
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch)\n",
    "    test(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
