{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* The following codes comes from the original DBPN project. \n",
    "* We are not going to directly train the model. Instead, based on the pretrained model,\n",
    "* We transfer the knowledge to our SRDectection Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from dbpn import Net as DBPN\n",
    "from dbpn_v1 import Net as DBPNLL\n",
    "from data import get_eval_set\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.misc import imsave\n",
    "import scipy.io as sio\n",
    "import time\n",
    "import cv2\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(chop_forward=False, gpu_mode=True, gpus=1, input_dir='../dataset', model='models/DBPN_x8.pth', model_type='DBPN', output='../dataset/results', seed=123, self_ensemble=False, testBatchSize=1, test_dataset='VOC12-LR-X8-restore', threads=1, upscale_factor=8)\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "parser.add_argument('--upscale_factor', type=int, default=8, help=\"super resolution upscale factor\")\n",
    "parser.add_argument('--testBatchSize', type=int, default=1, help='testing batch size')\n",
    "parser.add_argument('--gpu_mode', type=bool, default=True)\n",
    "parser.add_argument('--self_ensemble', type=bool, default=False)\n",
    "parser.add_argument('--chop_forward', type=bool, default=False)\n",
    "parser.add_argument('--threads', type=int, default=1, help='number of threads for data loader to use')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "parser.add_argument('--gpus', default=1, type=float, help='number of gpu')\n",
    "\n",
    "parser.add_argument('--input_dir', type=str, default='../dataset')\n",
    "parser.add_argument('--output', default='../dataset/results', help='Location to save checkpoint models')\n",
    "parser.add_argument('--test_dataset', type=str, default='VOC12-LR-X8-restore')\n",
    "parser.add_argument('--model_type', type=str, default='DBPN')\n",
    "parser.add_argument('--model', default='models/DBPN_x8.pth', help='sr pretrained base model')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "gpus_list=range(opt.gpus)\n",
    "print(opt)\n",
    "\n",
    "cuda = opt.gpu_mode\n",
    "if cuda and not torch.cuda.is_available():\n",
    "    raise Exception(\"No GPU found, please run without --cuda\")\n",
    "\n",
    "torch.manual_seed(opt.seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(opt.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loading datasets\n",
      "===> Building model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/detection/dbpn/dbpn.py:47: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  torch.nn.init.kaiming_normal(m.weight)\n",
      "/home/shared/detection/dbpn/dbpn.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  torch.nn.init.kaiming_normal(m.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained SR model is loaded.\n",
      "===> Processing: 2012_002263.jpg || Timer: 0.0418 sec.\n",
      "===> Processing: 2009_000746.jpg || Timer: 0.0388 sec.\n",
      "===> Processing: 2010_002312.jpg || Timer: 0.0083 sec.\n",
      "===> Processing: 2008_005616.jpg || Timer: 0.0081 sec.\n",
      "===> Processing: 2008_003323.jpg || Timer: 0.0100 sec.\n",
      "===> Processing: 2009_001110.jpg || Timer: 0.0081 sec.\n",
      "===> Processing: 2011_006531.jpg || Timer: 0.0206 sec.\n",
      "===> Processing: 2011_000207.jpg || Timer: 0.0082 sec.\n",
      "===> Processing: 2011_006864.jpg || Timer: 0.0081 sec.\n",
      "===> Processing: 2011_005395.jpg || Timer: 0.0080 sec.\n"
     ]
    }
   ],
   "source": [
    "print('===> Loading datasets')\n",
    "test_set = get_eval_set(os.path.join(opt.input_dir,opt.test_dataset))\n",
    "testing_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.testBatchSize, shuffle=False)\n",
    "\n",
    "print('===> Building model')\n",
    "if opt.model_type == 'DBPNLL':\n",
    "    model = DBPNLL(num_channels=3, base_filter=64,  feat = 256, num_stages=10, scale_factor=opt.upscale_factor) ##For NTIRE2018\n",
    "else:\n",
    "    model = DBPN(num_channels=3, base_filter=64,  feat = 256, num_stages=7, scale_factor=opt.upscale_factor) ###D-DBPN\n",
    "\n",
    "if cuda:\n",
    "    model = torch.nn.DataParallel(model, device_ids=gpus_list)\n",
    "\n",
    "if os.path.exists(opt.model):\n",
    "    #model= torch.load(opt.model, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(torch.load(opt.model, map_location=lambda storage, loc: storage))\n",
    "    print('Pre-trained SR model is loaded.')\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda(gpus_list[0])\n",
    "\n",
    "def eval():\n",
    "    model.eval()\n",
    "    for i,batch in enumerate(testing_data_loader):\n",
    "        input, name = batch[0], batch[1]\n",
    "        if cuda:\n",
    "            input = input.cuda(gpus_list[0])\n",
    "\n",
    "        t0 = time.time()\n",
    "        if opt.chop_forward:\n",
    "            prediction = chop_forward(input, model, opt.upscale_factor)\n",
    "        else:\n",
    "            if opt.self_ensemble:\n",
    "                prediction = x8_forward(input, model)\n",
    "            else:\n",
    "                prediction = model(input)\n",
    "        t1 = time.time()\n",
    "        print(\"===> Processing: %s || Timer: %.4f sec.\" % (name[0], (t1 - t0)))\n",
    "        save_img(prediction.cpu().data, name[0])\n",
    "        if i > 8:\n",
    "            break\n",
    "\n",
    "def save_img(img, img_name):\n",
    "    save_img = img.squeeze().clamp(0, 1).numpy().transpose(1,2,0)\n",
    "\n",
    "    # save img\n",
    "    save_dir=os.path.join(opt.output,opt.test_dataset)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    save_fn = save_dir +'/'+ img_name\n",
    "    cv2.imwrite(save_fn, cv2.cvtColor(save_img*255, cv2.COLOR_BGR2RGB),  [cv2.IMWRITE_PNG_COMPRESSION, 0])\n",
    "    cv2\n",
    "#####x8_forward and chop_forward taken from https://github.com/thstkdgus35/EDSR-PyTorch\n",
    "#####EDSR and MDSR Team from SNU\n",
    "def x8_forward(img, model, precision='single'):\n",
    "    def _transform(v, op):\n",
    "        if precision != 'single': v = v.float()\n",
    "\n",
    "        v2np = v.data.cpu().numpy()\n",
    "        if op == 'vflip':\n",
    "            tfnp = v2np[:, :, :, ::-1].copy()\n",
    "        elif op == 'hflip':\n",
    "            tfnp = v2np[:, :, ::-1, :].copy()\n",
    "        elif op == 'transpose':\n",
    "            tfnp = v2np.transpose((0, 1, 3, 2)).copy()\n",
    "        \n",
    "        #ret = torch.Tensor(tfnp).cuda()\n",
    "        ret = torch.Tensor(tfnp)\n",
    "\n",
    "        if precision == 'half':\n",
    "            ret = ret.half()\n",
    "        elif precision == 'double':\n",
    "            ret = ret.double()\n",
    "\n",
    "        return Variable(ret, volatile=v.volatile)\n",
    "\n",
    "    inputlist = [img]\n",
    "    for tf in 'vflip', 'hflip', 'transpose':\n",
    "        inputlist.extend([_transform(t, tf) for t in inputlist])\n",
    "\n",
    "    outputlist = [model(aug) for aug in inputlist]\n",
    "    for i in range(len(outputlist)):\n",
    "        if i > 3:\n",
    "            outputlist[i] = _transform(outputlist[i], 'transpose')\n",
    "        if i % 4 > 1:\n",
    "            outputlist[i] = _transform(outputlist[i], 'hflip')\n",
    "        if (i % 4) % 2 == 1:\n",
    "            outputlist[i] = _transform(outputlist[i], 'vflip')\n",
    "    \n",
    "    output = reduce((lambda x, y: x + y), outputlist) / len(outputlist)\n",
    "\n",
    "    return output\n",
    "    \n",
    "def chop_forward(x, model, scale, shave=16, min_size=10000, nGPUs=opt.gpus):\n",
    "    b, c, h, w = x.size()\n",
    "    h_half, w_half = h // 2, w // 2\n",
    "    h_size, w_size = h_half + shave, w_half + shave\n",
    "    inputlist = [\n",
    "        x[:, :, 0:h_size, 0:w_size],\n",
    "        x[:, :, 0:h_size, (w - w_size):w],\n",
    "        x[:, :, (h - h_size):h, 0:w_size],\n",
    "        x[:, :, (h - h_size):h, (w - w_size):w]]\n",
    "\n",
    "    if w_size * h_size < min_size:\n",
    "        outputlist = []\n",
    "        for i in range(0, 4, nGPUs):\n",
    "            input_batch = torch.cat(inputlist[i:(i + nGPUs)], dim=0)\n",
    "            if opt.self_ensemble:\n",
    "                output_batch = x8_forward(input_batch, model)\n",
    "            else:\n",
    "                output_batch = model(input_batch)\n",
    "            outputlist.extend(output_batch.chunk(nGPUs, dim=0))\n",
    "    else:\n",
    "        outputlist = [\n",
    "            chop_forward(patch, model, scale, shave, min_size, nGPUs) \\\n",
    "            for patch in inputlist]\n",
    "\n",
    "    h, w = scale * h, scale * w\n",
    "    h_half, w_half = scale * h_half, scale * w_half\n",
    "    h_size, w_size = scale * h_size, scale * w_size\n",
    "    shave *= scale\n",
    "\n",
    "    output = Variable(x.data.new(b, c, h, w), volatile=True)\n",
    "    output[:, :, 0:h_half, 0:w_half] \\\n",
    "        = outputlist[0][:, :, 0:h_half, 0:w_half]\n",
    "    output[:, :, 0:h_half, w_half:w] \\\n",
    "        = outputlist[1][:, :, 0:h_half, (w_size - w + w_half):w_size]\n",
    "    output[:, :, h_half:h, 0:w_half] \\\n",
    "        = outputlist[2][:, :, (h_size - h + h_half):h_size, 0:w_half]\n",
    "    output[:, :, h_half:h, w_half:w] \\\n",
    "        = outputlist[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]\n",
    "\n",
    "    return output\n",
    "\n",
    "##Eval Start!!!!\n",
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/results/VOC12-LR-X8-restore/2009_002624.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b225eee3275a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../dataset/results/VOC12-LR-X8-restore\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2009_002624.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2543\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2544\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/results/VOC12-LR-X8-restore/2009_002624.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_dir = \"../dataset/results/VOC12-LR-X8-restore\"\n",
    "img = Image.open(join(image_dir,\"2012_002263.jpg\"))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
